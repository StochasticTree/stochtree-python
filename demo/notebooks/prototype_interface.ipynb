{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of the `StochTree` Prototype Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the functions `bart()` and `bcf()` provide simple and performant \n",
    "interfaces for supervised learning / causal inference, `stochtree` also \n",
    "offers access to many of the \"low-level\" data structures that are typically \n",
    "implemented in C++.\n",
    "This low-level interface is not designed for performance or even\n",
    "simplicity --- rather the intent is to provide a \"prototype\" interface\n",
    "to the C++ code that doesn't require modifying any C++.\n",
    "\n",
    "To illustrate when such a prototype interface might be useful, consider\n",
    "the classic BART algorithm:\n",
    "\n",
    "**INPUT**: $y$, $X$, $\\tau$, $\\nu$, $\\lambda$, $\\alpha$, $\\beta$\n",
    "\n",
    "**OUTPUT**: $m$ samples of a decision forest with $k$ trees and global variance parameter $\\sigma^2$\n",
    "\n",
    "Initialize $\\sigma^2$ via a default or a data-dependent calibration exercise\n",
    "\n",
    "Initialize \"forest 0\" with $k$ trees with a single root node, referring to tree $j$'s prediction vector as $f_{0,j}$\n",
    "\n",
    "Compute residual as $r = y - \\sum_{j=1}^k f_{0,j}$\n",
    "\n",
    "**FOR** $i$ **IN** $\\left\\{1,\\dots,m\\right\\}$:\n",
    "\n",
    "    Initialize forest $i$ from forest $i-1$\n",
    "    \n",
    "    **FOR** $j$ **IN** $\\left\\{1,\\dots,k\\right\\}$:\n",
    "        \n",
    "        Add predictions for tree $j$ to residual: $r = r + f_{i,j}$ \n",
    "        \n",
    "        Update tree $j$ via Metropolis-Hastings with $r$ and $X$ as data and tree priors depending on ($\\tau$, $\\sigma^2$, $\\alpha$, $\\beta$)\n",
    "\n",
    "        Sample leaf node parameters for tree $j$ via Gibbs (leaf node prior is $N\\left(0,\\tau\\right)$)\n",
    "        \n",
    "        Subtract (updated) predictions for tree $j$ from residual: $r = r - f_{i,j}$\n",
    "\n",
    "        Sample $\\sigma^2$ via Gibbs (prior is $IG(\\nu/2,\\nu\\lambda/2)$)\n",
    "\n",
    "While the algorithm itself is conceptually simple, much of the core \n",
    "computation is carried out in low-level languages such as C or C++ \n",
    "because of the tree data structure. As a result, any changes to this \n",
    "algorithm, such as supporting heteroskedasticity (@pratola2020heteroscedastic), \n",
    "categorical outcomes (@murray2021log) or causal effect estimation (@hahn2020bayesian) \n",
    "require modifying low-level code. \n",
    "\n",
    "The prototype interface exposes the core components of the \n",
    "loop above at the R level, thus making it possible to interchange \n",
    "C++ computation for steps like \"update tree $j$ via Metropolis-Hastings\" \n",
    "with R computation for a custom variance model, other user-specified additive \n",
    "mean model components, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from stochtree import Dataset, Residual, RNG, ForestSampler, ForestContainer, GlobalVarianceModel, LeafVarianceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG\n",
    "random_seed = 1234\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "# Generate covariates and basis\n",
    "n = 1000\n",
    "p_X = 10\n",
    "p_W = 1\n",
    "X = rng.uniform(0, 1, (n, p_X))\n",
    "W = rng.uniform(0, 1, (n, p_W))\n",
    "\n",
    "# Define the outcome mean function\n",
    "def outcome_mean(X, W):\n",
    "    return np.where(\n",
    "        (X[:,0] >= 0.0) & (X[:,0] < 0.25), -7.5 * W[:,0], \n",
    "        np.where(\n",
    "            (X[:,0] >= 0.25) & (X[:,0] < 0.5), -2.5 * W[:,0], \n",
    "            np.where(\n",
    "                (X[:,0] >= 0.5) & (X[:,0] < 0.75), 2.5 * W[:,0], \n",
    "                7.5 * W[:,0]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Generate outcome\n",
    "epsilon = rng.normal(0, 1, n)\n",
    "y = outcome_mean(X, W) + epsilon\n",
    "\n",
    "# Standardize outcome\n",
    "y_bar = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "resid = (y-y_bar)/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some sampling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "beta = 1.25\n",
    "min_samples_leaf = 1\n",
    "num_trees = 100\n",
    "cutpoint_grid_size = 100\n",
    "global_variance_init = 1.\n",
    "tau_init = 0.5\n",
    "leaf_prior_scale = np.array([[tau_init]], order='C')\n",
    "nu = 4.\n",
    "lamb = 0.5\n",
    "a_leaf = 2.\n",
    "b_leaf = 0.5\n",
    "leaf_regression = True\n",
    "feature_types = np.repeat(0, p_X).astype(int) # 0 = numeric\n",
    "var_weights = np.repeat(1/p_X, p_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data from numpy to `StochTree` representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (covariates and basis)\n",
    "dataset = Dataset()\n",
    "dataset.add_covariates(X)\n",
    "dataset.add_basis(W)\n",
    "\n",
    "# Residual\n",
    "residual = Residual(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize tracking and sampling classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_container = ForestContainer(num_trees, W.shape[1], False)\n",
    "forest_sampler = ForestSampler(dataset, feature_types, num_trees, n, alpha, beta, min_samples_leaf)\n",
    "cpp_rng = RNG(random_seed)\n",
    "global_var_model = GlobalVarianceModel()\n",
    "leaf_var_model = LeafVarianceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to run the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmstart = 10\n",
    "num_mcmc = 100\n",
    "num_samples = num_warmstart + num_mcmc\n",
    "global_var_samples = np.concatenate((np.array([global_variance_init]), np.repeat(0, num_samples)))\n",
    "leaf_scale_samples = np.concatenate((np.array([tau_init]), np.repeat(0, num_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the \"grow-from-root\" (XBART) sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart):\n",
    "  forest_sampler.sample_one_iteration(forest_container, dataset, residual, cpp_rng, feature_types, cutpoint_grid_size, leaf_prior_scale, var_weights, global_var_samples[i], 1, True, False)\n",
    "  global_var_samples[i+1] = global_var_model.sample_one_iteration(residual, cpp_rng, nu, lamb)\n",
    "  leaf_scale_samples[i+1] = leaf_var_model.sample_one_iteration(forest_container, cpp_rng, a_leaf, b_leaf, i)\n",
    "  leaf_prior_scale[0,0] = leaf_scale_samples[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the MCMC (BART) sampler, initialized at the last XBART sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart, num_samples):\n",
    "  forest_sampler.sample_one_iteration(forest_container, dataset, residual, cpp_rng, feature_types, cutpoint_grid_size, leaf_prior_scale, var_weights, global_var_samples[i], 1, False, False)\n",
    "  global_var_samples[i+1] = global_var_model.sample_one_iteration(residual, cpp_rng, nu, lamb)\n",
    "  leaf_scale_samples[i+1] = leaf_var_model.sample_one_iteration(forest_container, cpp_rng, a_leaf, b_leaf, i)\n",
    "  leaf_prior_scale[0,0] = leaf_scale_samples[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract mean function and error variance posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest predictions\n",
    "forest_preds = forest_container.predict(dataset)*y_std + y_bar\n",
    "forest_preds_gfr = forest_preds[:,:num_warmstart]\n",
    "forest_preds_mcmc = forest_preds[:,num_warmstart:num_samples]\n",
    "\n",
    "# Global error variance\n",
    "sigma_samples = np.sqrt(global_var_samples)*y_std\n",
    "sigma_samples_gfr = sigma_samples[:num_warmstart]\n",
    "sigma_samples_mcmc = sigma_samples[num_warmstart:num_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the GFR (XBART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_gfr = forest_preds_gfr.mean(axis = 1, keepdims = True)\n",
    "forest_pred_df_gfr = pd.DataFrame(np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_gfr), axis = 1), columns=[\"True y\", \"Average predicted y\"])\n",
    "sns.scatterplot(data=forest_pred_df_gfr, x=\"True y\", y=\"Average predicted y\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_gfr = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(num_warmstart),axis=1), np.expand_dims(sigma_samples_gfr,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\n",
    "sns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the MCMC (BART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_mcmc = forest_preds_mcmc.mean(axis = 1, keepdims = True)\n",
    "forest_pred_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(y, axis=1), forest_pred_avg_mcmc), axis = 1), columns=[\"True y\", \"Average predicted y\"])\n",
    "sns.scatterplot(data=forest_pred_df_mcmc, x=\"True y\", y=\"Average predicted y\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(num_samples - num_warmstart),axis=1), np.expand_dims(sigma_samples_mcmc,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\n",
    "sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Causal Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG\n",
    "random_seed = 101\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "# Generate covariates and basis\n",
    "n = 1000\n",
    "p_X = 5\n",
    "X = rng.uniform(0, 1, (n, p_X))\n",
    "pi_X = 0.25 + 0.5*X[:,0]\n",
    "Z = rng.binomial(1, pi_X, n).astype(float)\n",
    "\n",
    "# Define the outcome mean functions (prognostic and treatment effects)\n",
    "mu_X = pi_X*5\n",
    "# tau_X = np.sin(X[:,1]*2*np.pi)\n",
    "tau_X = X[:,1]*2\n",
    "\n",
    "# Generate outcome\n",
    "epsilon = rng.normal(0, 1, n)\n",
    "y = mu_X + tau_X*Z + epsilon\n",
    "\n",
    "# Standardize outcome\n",
    "y_bar = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "resid = (y-y_bar)/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some sampling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognostic forest parameters\n",
    "alpha_mu = 0.95\n",
    "beta_mu = 2.0\n",
    "min_samples_leaf_mu = 1\n",
    "num_trees_mu = 200\n",
    "cutpoint_grid_size_mu = 100\n",
    "tau_init_mu = 1/200\n",
    "leaf_prior_scale_mu = np.array([[tau_init_mu]], order='C')\n",
    "a_leaf_mu = 3.\n",
    "b_leaf_mu = 1/200\n",
    "leaf_regression_mu = False\n",
    "feature_types_mu = np.repeat(0, p_X).astype(int) # 0 = numeric\n",
    "var_weights_mu = np.repeat(1/(p_X + 1), p_X + 1)\n",
    "\n",
    "# Treatment forest parameters\n",
    "alpha_tau = 0.25\n",
    "beta_tau = 3.\n",
    "min_samples_leaf_tau = 1\n",
    "num_trees_tau = 50\n",
    "cutpoint_grid_size_tau = 100\n",
    "tau_init_tau = 1/50\n",
    "leaf_prior_scale_tau = np.array([[tau_init_tau]], order='C')\n",
    "a_leaf_tau = 3.\n",
    "b_leaf_tau = 1/50\n",
    "leaf_regression_tau = True\n",
    "feature_types_tau = np.repeat(0, p_X).astype(int) # 0 = numeric\n",
    "var_weights_tau = np.repeat(1/p_X, p_X)\n",
    "\n",
    "# Global parameters\n",
    "nu = 2.\n",
    "lamb = 0.5\n",
    "global_variance_init = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data from numpy to `StochTree` representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognostic Forest Dataset (covariates)\n",
    "dataset_mu = Dataset()\n",
    "dataset_mu.add_covariates(np.c_[X,pi_X])\n",
    "\n",
    "# Treatment Forest Dataset (covariates and treatment variable)\n",
    "dataset_tau = Dataset()\n",
    "dataset_tau.add_covariates(X)\n",
    "dataset_tau.add_basis(Z)\n",
    "\n",
    "# Residual\n",
    "residual = Residual(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize tracking and sampling classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prognostic forest sampling classes\n",
    "forest_container_mu = ForestContainer(num_trees_mu, 1, True)\n",
    "forest_sampler_mu = ForestSampler(dataset_mu, feature_types_mu, num_trees_mu, n, alpha_mu, beta_mu, min_samples_leaf_mu)\n",
    "leaf_var_model_mu = LeafVarianceModel()\n",
    "\n",
    "# Treatment forest sampling classes\n",
    "forest_container_tau = ForestContainer(num_trees_tau, 1 if np.ndim(Z) == 1 else Z.shape[1], False)\n",
    "forest_sampler_tau = ForestSampler(dataset_tau, feature_types_tau, num_trees_tau, n, alpha_tau, beta_tau, min_samples_leaf_tau)\n",
    "leaf_var_model_tau = LeafVarianceModel()\n",
    "\n",
    "# Global classes\n",
    "cpp_rng = RNG(random_seed)\n",
    "global_var_model = GlobalVarianceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to run the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmstart = 10\n",
    "num_mcmc = 500\n",
    "num_samples = num_warmstart + num_mcmc\n",
    "global_var_samples = np.concatenate((np.array([global_variance_init]), np.repeat(0, num_samples)))\n",
    "leaf_scale_samples_mu = np.concatenate((np.array([tau_init_mu]), np.repeat(0, num_samples)))\n",
    "leaf_scale_samples_tau = np.concatenate((np.array([tau_init_tau]), np.repeat(0, num_samples)))\n",
    "leaf_prior_scale_mu = np.array([[tau_init_mu]])\n",
    "leaf_prior_scale_tau = np.array([[tau_init_tau]])\n",
    "b_0_init = -0.5\n",
    "b_1_init = 0.5\n",
    "b_0_samples = np.concatenate((np.array([b_0_init]), np.repeat(0, num_samples)))\n",
    "b_1_samples = np.concatenate((np.array([b_1_init]), np.repeat(0, num_samples)))\n",
    "tau_basis = (1-Z)*b_0_init + Z*b_1_init\n",
    "dataset_tau.update_basis(tau_basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the \"grow-from-root\" (XBART) sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart):\n",
    "  # Sample the prognostic forest\n",
    "  forest_sampler_mu.sample_one_iteration(forest_container_mu, dataset_mu, residual, cpp_rng, feature_types_mu, cutpoint_grid_size_mu, leaf_prior_scale_mu, var_weights_mu, global_var_samples[i], 0, True, False)\n",
    "  leaf_scale_samples_mu[i+1] = leaf_var_model_mu.sample_one_iteration(forest_container_mu, cpp_rng, a_leaf_mu, b_leaf_mu, i)\n",
    "  leaf_prior_scale_mu[0,0] = leaf_scale_samples_mu[i+1]\n",
    "  mu_x = forest_container_mu.predict_raw_single_forest(dataset_mu, i)\n",
    "\n",
    "  # Sample the treatment effect forest\n",
    "  forest_sampler_tau.sample_one_iteration(forest_container_tau, dataset_tau, residual, cpp_rng, feature_types_tau, cutpoint_grid_size_tau, leaf_prior_scale_tau, var_weights_tau, global_var_samples[i], 1, True, False)\n",
    "  # leaf_scale_samples_tau[i+1] = leaf_var_model_tau.sample_one_iteration(forest_container_tau, cpp_rng, a_leaf_tau, b_leaf_tau, i)\n",
    "  # leaf_prior_scale_tau[0,0] = leaf_scale_samples_tau[i+1]\n",
    "  tau_x = np.squeeze(forest_container_tau.predict_raw_single_forest(dataset_tau, i))\n",
    "  s_tt0 = np.sum(tau_x*tau_x*(Z==0))\n",
    "  s_tt1 = np.sum(tau_x*tau_x*(Z==1))\n",
    "  partial_resid_mu = resid - np.squeeze(mu_x)\n",
    "  s_ty0 = np.sum(tau_x*partial_resid_mu*(Z==0))\n",
    "  s_ty1 = np.sum(tau_x*partial_resid_mu*(Z==1))\n",
    "  b_0_samples[i+1] = rng.normal(loc = (s_ty0/(s_tt0 + 2*global_var_samples[i])), scale = np.sqrt(global_var_samples[i]/(s_tt0 + 2*global_var_samples[i])), size = 1)\n",
    "  b_1_samples[i+1] = rng.normal(loc = (s_ty1/(s_tt1 + 2*global_var_samples[i])), scale = np.sqrt(global_var_samples[i]/(s_tt1 + 2*global_var_samples[i])), size = 1)\n",
    "  tau_basis = (1-Z)*b_0_samples[i+1] + Z*b_1_samples[i+1]\n",
    "  dataset_tau.update_basis(tau_basis)\n",
    "  \n",
    "  # Sample global variance\n",
    "  global_var_samples[i+1] = global_var_model.sample_one_iteration(residual, cpp_rng, nu, lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the MCMC (BART) sampler, initialized at the last XBART sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_warmstart, num_samples):\n",
    "  # Sample the prognostic forest\n",
    "  forest_sampler_mu.sample_one_iteration(forest_container_mu, dataset_mu, residual, cpp_rng, feature_types_mu, cutpoint_grid_size_mu, leaf_prior_scale_mu, var_weights_mu, global_var_samples[i], 0, False, False)\n",
    "  leaf_scale_samples_mu[i+1] = leaf_var_model_mu.sample_one_iteration(forest_container_mu, cpp_rng, a_leaf_mu, b_leaf_mu, i)\n",
    "  leaf_prior_scale_mu[0,0] = leaf_scale_samples_mu[i+1]\n",
    "  mu_x = forest_container_mu.predict_raw_single_forest(dataset_mu, i)\n",
    "\n",
    "  # Sample the treatment effect forest\n",
    "  forest_sampler_tau.sample_one_iteration(forest_container_tau, dataset_tau, residual, cpp_rng, feature_types_tau, cutpoint_grid_size_tau, leaf_prior_scale_tau, var_weights_tau, global_var_samples[i], 1, False, False)\n",
    "  # leaf_scale_samples_tau[i+1] = leaf_var_model_tau.sample_one_iteration(forest_container_tau, cpp_rng, a_leaf_tau, b_leaf_tau, i)\n",
    "  # leaf_prior_scale_tau[0,0] = leaf_scale_samples_tau[i+1]\n",
    "  tau_x = np.squeeze(forest_container_tau.predict_raw_single_forest(dataset_tau, i))\n",
    "  s_tt0 = np.sum(tau_x*tau_x*(Z==0))\n",
    "  s_tt1 = np.sum(tau_x*tau_x*(Z==1))\n",
    "  partial_resid_mu = resid - np.squeeze(mu_x)\n",
    "  s_ty0 = np.sum(tau_x*partial_resid_mu*(Z==0))\n",
    "  s_ty1 = np.sum(tau_x*partial_resid_mu*(Z==1))\n",
    "  b_0_samples[i+1] = rng.normal(loc = (s_ty0/(s_tt0 + 2*global_var_samples[i])), scale = np.sqrt(global_var_samples[i]/(s_tt0 + 2*global_var_samples[i])), size = 1)\n",
    "  b_1_samples[i+1] = rng.normal(loc = (s_ty1/(s_tt1 + 2*global_var_samples[i])), scale = np.sqrt(global_var_samples[i]/(s_tt1 + 2*global_var_samples[i])), size = 1)\n",
    "  tau_basis = (1-Z)*b_0_samples[i+1] + Z*b_1_samples[i+1]\n",
    "  dataset_tau.update_basis(tau_basis)\n",
    "  \n",
    "  # Sample global variance\n",
    "  global_var_samples[i+1] = global_var_model.sample_one_iteration(residual, cpp_rng, nu, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_container_tau.predict_raw(dataset_tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract mean function and error variance posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest predictions\n",
    "forest_preds_mu = forest_container_mu.predict(dataset_mu)*y_std + y_bar\n",
    "forest_preds_mu_gfr = forest_preds_mu[:,:num_warmstart]\n",
    "forest_preds_mu_mcmc = forest_preds_mu[:,num_warmstart:num_samples]\n",
    "treatment_coding_samples = (b_1_samples[1:] - b_0_samples[1:])\n",
    "forest_preds_tau = (forest_container_tau.predict_raw(dataset_tau)*y_std*np.expand_dims(treatment_coding_samples, axis=(0,2)))\n",
    "forest_preds_tau_gfr = forest_preds_tau[:,:num_warmstart]\n",
    "forest_preds_tau_mcmc = forest_preds_tau[:,num_warmstart:num_samples]\n",
    "\n",
    "# Global error variance\n",
    "sigma_samples = np.sqrt(global_var_samples)*y_std\n",
    "sigma_samples_gfr = sigma_samples[:num_warmstart]\n",
    "sigma_samples_mcmc = sigma_samples[num_warmstart:num_samples]\n",
    "\n",
    "# Adaptive coding parameters\n",
    "b_1_samples_gfr = b_1_samples[1:(num_warmstart+1)]*y_std\n",
    "b_0_samples_gfr = b_0_samples[1:(num_warmstart+1)]*y_std\n",
    "b_1_samples_mcmc = b_1_samples[(num_warmstart+1):]*y_std\n",
    "b_0_samples_mcmc = b_0_samples[(num_warmstart+1):]*y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the GFR (XBART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_preds_tau_avg_gfr = np.squeeze(forest_preds_tau_gfr).mean(axis = 1, keepdims = True)\n",
    "forest_pred_tau_df_gfr = pd.DataFrame(np.concatenate((np.expand_dims(tau_X,1), forest_preds_tau_avg_gfr), axis = 1), columns=[\"True tau\", \"Average estimated tau\"])\n",
    "sns.scatterplot(data=forest_pred_tau_df_gfr, x=\"True tau\", y=\"Average estimated tau\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_gfr = np.squeeze(forest_preds_mu_gfr).mean(axis = 1, keepdims = True)\n",
    "forest_pred_df_gfr = pd.DataFrame(np.concatenate((np.expand_dims(tau_X,1), forest_pred_avg_gfr), axis = 1), columns=[\"True mu\", \"Average estimated mu\"])\n",
    "sns.scatterplot(data=forest_pred_df_gfr, x=\"True mu\", y=\"Average estimated mu\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_gfr = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(num_warmstart),axis=1), np.expand_dims(sigma_samples_gfr,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\n",
    "sns.scatterplot(data=sigma_df_gfr, x=\"Sample\", y=\"Sigma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df_gfr = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(num_warmstart),axis=1), np.expand_dims(b_0_samples_gfr,axis=1), np.expand_dims(b_1_samples_gfr,axis=1)), axis = 1), columns=[\"Sample\", \"Beta_0\", \"Beta_1\"])\n",
    "sns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_0\")\n",
    "sns.scatterplot(data=b_df_gfr, x=\"Sample\", y=\"Beta_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the MCMC (BART) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_mcmc = np.squeeze(forest_preds_tau_mcmc).mean(axis = 1, keepdims = True)\n",
    "forest_pred_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(tau_X,1), forest_pred_avg_mcmc), axis = 1), columns=[\"True tau\", \"Average estimated tau\"])\n",
    "sns.scatterplot(data=forest_pred_df_mcmc, x=\"True tau\", y=\"Average estimated tau\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred_avg_mcmc = np.squeeze(forest_preds_mu_mcmc).mean(axis = 1, keepdims = True)\n",
    "forest_pred_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(tau_X,1), forest_pred_avg_mcmc), axis = 1), columns=[\"True mu\", \"Average estimated mu\"])\n",
    "sns.scatterplot(data=forest_pred_df_mcmc, x=\"True mu\", y=\"Average estimated mu\")\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(num_samples - num_warmstart),axis=1), np.expand_dims(sigma_samples_mcmc,axis=1)), axis = 1), columns=[\"Sample\", \"Sigma\"])\n",
    "sns.scatterplot(data=sigma_df_mcmc, x=\"Sample\", y=\"Sigma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df_mcmc = pd.DataFrame(np.concatenate((np.expand_dims(np.arange(num_samples - num_warmstart),axis=1), np.expand_dims(b_0_samples_mcmc,axis=1), np.expand_dims(b_1_samples_mcmc,axis=1)), axis = 1), columns=[\"Sample\", \"Beta_0\", \"Beta_1\"])\n",
    "sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_0\")\n",
    "sns.scatterplot(data=b_df_mcmc, x=\"Sample\", y=\"Beta_1\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochtree-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
